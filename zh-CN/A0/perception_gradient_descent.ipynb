{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "感知机和梯度下降示例\n",
    "===========\n",
    "\n",
    "本示例主要是用代码展示感知机和梯度下降是如何工作的。\n",
    "\n",
    "本示例参考自https://www.youtube.com/watch?v=kft1AJ9WVDk\n",
    "\n",
    "这里，我们需要求解的问题是：\n",
    "\n",
    "![problemset](problemset.png)\n",
    "\n",
    "一共有4个样例，每个样例的输入是一个3维实数向量，输出一个实数。然后给出一个新的输入，求解其输出。\n",
    "\n",
    "这个问题比较特殊，一个直接的解就是输出等于输入的第一个维度的值，可以看到这样能够完美拟合所有的样例。\n",
    "\n",
    "考虑使用感知机来对本问题进行求解，假定我们可以使用如下的函数来进行拟合：\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\phi( \\sum_{i=1}^{3} x_i w_i )\n",
    "$$\n",
    "\n",
    "其中$\\hat{y}$是感知机的输出，$\\phi$是sigmoid函数，$\\phi(x) = \\frac{1}{1+e^{-x}}$，$x_i$是一个输入，$w_i$是我们需要寻找的参数。\n",
    "\n",
    "上面的感知机画成图的话如下：\n",
    "\n",
    "![perception](perception.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0xff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([\n",
    "    [0, 0, 1],\n",
    "    [1, 1, 1],\n",
    "    [1, 0, 1],\n",
    "    [0, 1, 1],\n",
    "])\n",
    "y = np.array([\n",
    "    0, 1, 1, 0\n",
    "])\n",
    "new_x = np.array([\n",
    "    1, 0, 0\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设置数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.46393652, 0.22303538, 0.32222402])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = np.random.rand(3)\n",
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "随机初始化参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.57986617, 0.73286276, 0.68700633, 0.63303504])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "sigmoid(np.dot(x, w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义$\\phi$函数，并打印当前感知机输出，可以看到与标准答案相去甚远。\n",
    "\n",
    "为了简化表达式，我们将感知机写成矩阵乘法的形式，即：\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\phi( x w )\n",
    "$$\n",
    "\n",
    "可以看到，如果我们调整参数$w$，那么自然感知机的输出也会随之改变。下一步我们需要定义什么样的参数才是“好的”，只有定义了什么样的参数是“好的”的情况下，我们才能够使用计算机进行求解。\n",
    "\n",
    "一个符合我们直觉的定义是，“能够使感知机的输出距离标准样例输出尽量近”的参数是“好的”，此处我们使用L2距离的平方来刻画“近”，因此转化成计算机能够求解的描述就是：\n",
    "\n",
    "**寻找参数$w$，使得以下函数的值最小：**\n",
    "\n",
    "$$\n",
    "loss = (\\phi(x w) - y)^2\n",
    "$$\n",
    "\n",
    "其中$x$是一个样例的标准输入，$y$是该样例对应的标准输出。\n",
    "\n",
    "上面的公式只考虑了一个样例，因此也可以将所有样例都考虑进来，一个普遍的做法就是求和：\n",
    "\n",
    "$$\n",
    "loss(w) = \\sum_{(x,y) \\in samples} (\\phi(x w) - y)^2\n",
    "$$\n",
    "\n",
    "这个函数在机器学习中一般被称作“损失函数”，下面考虑如何使用梯度下降算法进行求解。\n",
    "\n",
    "使用梯度下降算法求解一个函数的最小值，关键点在于求解**目标函数关于每个参数的导数**（对微积分又爱又恨，不过这里只需要用到微分的内容）。\n",
    "\n",
    "在机器学习中，**并不需要给出导数的解析式，只用给出一个计算导数的方法即可**，因此我们可以将$loss(w)$的梯度写成：\n",
    "\n",
    "$$\n",
    "\\frac{\\partial loss(w)}{\\partial w} = \\sum_{(x,y) \\in samples} 2 \\times (\\phi(x w) - y)  \\times \\frac{\\partial{\\phi(x w)}}{\\partial(x w)} \\times x^T\n",
    "$$\n",
    "\n",
    "其中，sigmoid函数的导数等于：\n",
    "\n",
    "$$\n",
    "\\frac{\\partial{\\phi(x w)}}{\\partial(x w)} = \\phi(x w)(1 - \\phi(x w))\n",
    "$$\n",
    "\n",
    "注：实际上在机器学习中，由于任何复杂的表达式都能够拆分成若干基本运算的组合，根据微积分中的链式法则，如果不考虑运行效率的话，只用分析每个基本操作的求导运算，然后按照链式法则进行组合即可求出导数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01 # 学习率\n",
    "steps = 10000 # 迭代数量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设定梯度下降相关参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = [0 1 1 0]\n",
      "step = 0, loss = 0.9063054746879384, output = [0.57986617 0.73286276 0.68700633 0.63303504]\n",
      "step = 1000, loss = 0.2061349606446641, output = [0.27465455 0.78242415 0.82219817 0.22747988]\n",
      "step = 2000, loss = 0.09944087387692463, output = [0.19279282 0.84612958 0.87817166 0.15412188]\n",
      "step = 3000, loss = 0.0628778921502277, output = [0.15348426 0.87707253 0.90317426 0.12179527]\n",
      "step = 4000, loss = 0.045231869350196484, output = [0.13010644 0.89555653 0.91774125 0.10309821]\n",
      "step = 5000, loss = 0.03503469184505395, output = [0.11440305 0.9080099  0.92746486 0.09068136]\n",
      "step = 6000, loss = 0.028456772357978083, output = [0.10301124 0.91706465 0.93450938 0.08172002]\n",
      "step = 7000, loss = 0.02388826170423647, output = [0.09430083 0.92400075 0.93989922 0.07488408]\n",
      "step = 8000, loss = 0.02054297320013291, output = [0.08738197 0.92951852 0.94418673 0.06945936]\n",
      "step = 9000, loss = 0.01799420573657285, output = [0.08172556 0.93403522 0.94769817 0.06502542]\n"
     ]
    }
   ],
   "source": [
    "print(f'y = {y}')\n",
    "for step in range(steps):\n",
    "    output = sigmoid(np.dot(x, w)) # 计算输出\n",
    "    loss = np.sum((output - y) * (output - y)) # 计算\n",
    "    gradient = 2 * (output - y) * output * (1 - output) * np.transpose(x)\n",
    "    gradient = np.sum(gradient, axis=1) # 计算梯度\n",
    "    \n",
    "    w = w - learning_rate * gradient # 更新\n",
    "    if step % 1000 == 0:\n",
    "        print(f'step = {step}, loss = {loss}, output = {output}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w = [ 5.44187185 -0.24453529 -2.48388091], new_x = [1 0 0], new_y = 0.9956873118323762\n"
     ]
    }
   ],
   "source": [
    "new_y = sigmoid(np.dot(new_x, w))\n",
    "\n",
    "print(f'w = {w}, new_x = {new_x}, new_y = {new_y}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，通过梯度下降算法，参数$w$已经收敛到了一个较好的结果，而新输入$[1, 0, 0]$的输出为$0.995$，与我们预想的输出$1$已经非常接近了。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
